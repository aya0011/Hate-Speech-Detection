# -*- coding: utf-8 -*-
"""Copy of Hate Speech detection project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OEBLOE94JLsV49fTXCryJpr1ebSKFjDN
"""

! pip install transformers tensorflow pandas scikit-learn emoji contractions matplotlib nltk seaborn

import pandas as pd
import numpy as np
import re, emoji, contractions
import nltk
import seaborn as sns
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.utils.class_weight import compute_class_weight
import matplotlib.pyplot as plt
import tensorflow as tf
from transformers import BertTokenizer, TFBertForSequenceClassification

nltk.download('stopwords')
nltk.download('wordnet')

df = pd.read_csv("/content/labeled_data.csv")
df = df[['tweet', 'class']]
df = df.rename(columns={'tweet':'text','class':'label'})

#preprocessing

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def clean_text(text):
    text = str(text).lower()
    text = contractions.fix(text)
    text = emoji.replace_emoji(text, replace='')
    text = re.sub(r"http\S+|www\S+|https\S+", '', text)
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    tokens = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]
    return ' '.join(tokens)

df['text'] = df['text'].apply(clean_text)

df.size

le=LabelEncoder()
df['label']=le.fit_transform(df['label'])

# 80% Train + 20% Temp (Validation + Test)
train_texts, temp_texts, train_labels, temp_labels = train_test_split(
    df['text'], df['label'], test_size=0.2, random_state=42, stratify=df['label']
)

# Split 20% Temp into 50% Validation and 50% Test
val_texts, test_texts, val_labels, test_labels = train_test_split(
    temp_texts, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels
)

#Tokenization (BERT)

MODEL_NAME = "bert-base-uncased"

avg_len = int(df['text'].str.split().apply(len).mean())
MAX_LEN = min(200, avg_len * 2)

tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)

train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=MAX_LEN, return_tensors='tf')
val_encodings = tokenizer(list(val_texts), truncation=True, padding=True, max_length=MAX_LEN, return_tensors='tf')
test_encodings = tokenizer(list(test_texts), truncation=True, padding=True, max_length=MAX_LEN, return_tensors='tf')

train_labels = tf.convert_to_tensor(train_labels)
val_labels = tf.convert_to_tensor(val_labels)
test_labels = tf.convert_to_tensor(test_labels)

model = TFBertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(le.classes_), from_pt=True)

# Full fine-tuning: all layers trainable
for layer in model.layers:
    layer.trainable = True

# Compile Model

optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metrics = ['accuracy']

model.compile(optimizer=optimizer, loss=loss, metrics=metrics)

#class weights for handle imbalance

class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(df['label']), y=df['label'])
class_weights = dict(enumerate(class_weights))

#Callbacks

callbacks = [
    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),
    tf.keras.callbacks.ModelCheckpoint('best_model', monitor='val_loss', save_best_only=True, save_format='tf')
]

# Convert encodings + labels to tf.data.Dataset
train_dataset = tf.data.Dataset.from_tensor_slices((
    {
        'input_ids': train_encodings['input_ids'],
        'attention_mask': train_encodings['attention_mask']
    },
    train_labels)).shuffle(1000).batch(16)

val_dataset = tf.data.Dataset.from_tensor_slices((
    {
        'input_ids': val_encodings['input_ids'],
        'attention_mask': val_encodings['attention_mask']
    },
    val_labels)).batch(16)

# Train model

history = model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=10,
    callbacks=callbacks,
    class_weight=class_weights
)

# Evaluate on Validation
val_dataset = tf.data.Dataset.from_tensor_slices((
    {'input_ids': val_encodings['input_ids'], 'attention_mask': val_encodings['attention_mask']},
    val_labels
)).batch(16)

val_pred = np.argmax(model.predict(val_dataset).logits, axis=1)

# Evaluate on Test
test_dataset = tf.data.Dataset.from_tensor_slices((
    {'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask']},
    test_labels
)).batch(16)

test_pred = np.argmax(model.predict(test_dataset).logits, axis=1)

# Reports
from sklearn.metrics import classification_report

print("\nValidation Report:")
val_report = classification_report(val_labels, val_pred, target_names=class_names, output_dict=True)
print(classification_report(val_labels, val_pred, target_names=class_names))

print("\nTest Report:")
test_report = classification_report(test_labels, test_pred, target_names=class_names, output_dict=True)
print(classification_report(test_labels, test_pred, target_names=class_names))

# Average F1
avg_f1 = (val_report['weighted avg']['f1-score'] + test_report['weighted avg']['f1-score']) / 2
print(f"\nAverage F1 (Val+Test): {avg_f1:.4f}")

plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Loss Curve')
plt.legend()

plt.subplot(1,2,2)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.title('Accuracy Curve')
plt.legend()
plt.show()

# Prediction on New Text


def predict_text(text):
    text_clean = clean_text(text)
    enc = tokenizer([text_clean], truncation=True, padding=True, max_length=MAX_LEN, return_tensors='tf')
    logits = model(enc).logits
    pred_index = np.argmax(logits.numpy(), axis=1)[0]
    pred_label = class_names[pred_index]
    return pred_label

examples = [
    "I hate those people!",
    "You are amazing!",
    "I don't like this at all.",
    "What a beautiful day!",
    "You are so stupid!",
]

for text in examples:
    print(f"Text: {text}")
    print("Prediction:", predict_text(text))
    print("-" * 50)